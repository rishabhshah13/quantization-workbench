import torch


class Inference:

    def __init__(model,device):
        self.model = model
        self.device = device #choice of device from the user


    def check_memory():
        ## check for memory usage
        ## check how many models can be loaded parallely or how many can be loaded sequentially
        pass    
        
        
    def inference(model,text):
        # do model inference here





    

